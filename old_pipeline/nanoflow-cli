#!/usr/bin/env python3

### Requirements: snakemake
###################################################################################################

import os
os.environ['ANSIBLE_FORCE_COLOR'] = "TRUE"
import sys
import platform
import argparse
import logging
import subprocess
import re
from pathlib import Path
import datetime
import glob
import time
import pandas as pd
import shutil
import gzip
from Bio import SeqIO
import threading

scriptPath = os.path.dirname(os.path.abspath(__file__))
print(scriptPath)
home = str(Path.home())
All = False
nowTime = datetime.datetime.now().strftime('%Y.%m.%d-%Hh%Mm%Ss')
nowTimeDay = datetime.datetime.now().strftime('%Y.%m.%d')
myPlatform = platform.system()

def output_reader(proc):
    for line in iter(proc.stdout.readline, b''):
        print('got line: {0}'.format(line.decode('utf-8')), end='')

parser = argparse.ArgumentParser(prog='nanoflow', 
  description='By default nanoflow performs a full analysis: demultiplex, clean human and rodent, assemble, map to the target pathogen, consensus', 
  formatter_class=lambda prog: argparse.HelpFormatter(prog,max_help_position=30,width=200))
requiredArgs = parser.add_argument_group('Required arguments')
requiredArgs.add_argument('-r', '--runid', action='store', dest='runid', type=str, required=True, metavar='RUNID')
optionalArgs = parser.add_argument_group('Optional arguments')
optionalArgs.add_argument('-i', '--input', action='store', dest='fastqDir', nargs='*',type=str, metavar='DIR', help='Input directory with basecalled fastq files | default: '+home+'/Desktop/NGS-data', default=home+'/Desktop/NGS-data')
optionalArgs.add_argument('-o', '--output', action='store', dest='outputFolder', type=str, metavar='DIR', help='Output directory | default: '+home+'/Desktop/NGS-data', default=home+'/Desktop/NGS-data')
optionalArgs.add_argument('-t', '--threads', action='store', dest='threads', type=int, metavar='INT', help='Number of threads | default: 4', default=4)
optionalArgs.add_argument('-s', '--sample', action='store', dest='selectedSamples', type=str, metavar='barcodeXX', help='Space separated selected barcodes', nargs='*')
optionalArgs.add_argument('-p', '--target', action='store', dest='targetPathogen', nargs='*', type=str, metavar='STR', help='Space separated selected target pathogen. Supported: LASV, ALL | default: LASV', default=['LASV'])
optionalArgs.add_argument('-m', '--ref','--manualRef', action='store', type=str, dest='manualRef', nargs='*', help='Manual mapping to custom reference')
optionalArgs.add_argument('-db', '--db', action='store', dest='db', type=str, metavar='DIR', help='Database directory | default: '+home+'/Desktop/NGS-scripts/DB', default=home+'/Desktop/NGS-scripts/DB')
optionalArgs.add_argument('-barcode-kit', '--barcode-kit', action='store', dest='barcodeKit',nargs='*', type=str,  help='Barcode kit | default: EXP-NBD104 EXP-NBD114', default=['EXP-NBD104', 'EXP-NBD114'])


optionalArgs.add_argument('-1', '--demultiplex', action='store_true', help='Demultiplex')
optionalArgs.add_argument('-2', '--trim', action='store_true', help='Trim')
optionalArgs.add_argument('--classify', action='store_true', help='Classify')

optionalArgs.add_argument('-3', '--clean', action ='store_true', help='Remove contaminant reads, default: reagents,human RNA, human DNA use --clean_option to change options  default reagents,human')
optionalArgs.add_argument('--clean_option', dest='cleanOptions', nargs='*',type=str, default=['reagents', 'human'], help='Specify order and which contaminant reads to remove among: reagents,human,mouse,mastomys,bacteria,arachnea | default: reagent, human')

optionalArgs.add_argument('-4', '--map', action='store_true', help='Map against target pathogen library')
optionalArgs.add_argument('-5', '--assemble', action='store_true', help='De Novo assembly of a 100000 reads subset')
optionalArgs.add_argument('-6', '--consensus', action='store_true', help='Generation of the consensus')
optionalArgs.add_argument('--assembler', action='store', dest='assembler', nargs='*', type=str, metavar='STR', help='Assembler: canu, flye or wtdbg2', default=['canu'])
optionalArgs.add_argument('-l','--label', action='store', dest='label', type=str, metavar='STR', help='Label the analysis run with extra info if planning to run several times', default='')
optionalArgs.add_argument('-size','-genomeSize', action='store', dest='genomeSize', type=str, metavar='INT', help='Best guess for genome size, default=10000', default=10000)
optionalArgs.add_argument('-cov','-covLimit', action='store', dest='covLimit', nargs='*', type=str, metavar='INT', help='Coverage limit for calling a consensus base', default=[20])
optionalArgs.add_argument('--keep_bam', action='store_true', dest='keep_bam', help='Generate the bam files, MUST be used while specifying sample with -s and ref with --ref')
optionalArgs.add_argument('--norm_fastq', action='store', dest='norm_fastq', nargs='*', type=str, help='Take a (list of) trimmed fastq file(s) as input for the pipeline. Intended to be used after normalisation')

inputArgs = parser.parse_args()

if (inputArgs.label):
  RUNID = inputArgs.runid + '-'+inputArgs.label
else:
  RUNID = inputArgs.runid


ANALYSIS_FOLDER = inputArgs.outputFolder + '/' + RUNID + '_ANALYSIS'
os.makedirs(ANALYSIS_FOLDER,exist_ok=True)

cleanOpts = re.sub('human','human_rna,human_dna',((',').join(inputArgs.cleanOptions)))
cleanOpts = re.sub('reagents','reagent',cleanOpts)

with open(ANALYSIS_FOLDER+'/config.yaml', 'w') as f:
  f.write('runid: ' + RUNID + '\n')
  f.write('table: ' + ANALYSIS_FOLDER + '/table.csv\n')
  f.write('data: ' + str(inputArgs.fastqDir) + '\n')
  f.write('demultiplex_path: ' + ANALYSIS_FOLDER + '/' + RUNID + '-01-demultiplex\n')
  f.write('trim_path: ' + ANALYSIS_FOLDER + '/' + RUNID + '-02-trim\n')
  f.write('clean_path: ' + ANALYSIS_FOLDER + '/' + RUNID + '-03-clean\n')
  if (inputArgs.cleanOptions):
    f.write('clean_option: ' + cleanOpts + '\n')
  f.write('map_path: ' + ANALYSIS_FOLDER + '/' + RUNID + '-04-map\n')
  f.write('assemble_path: ' + ANALYSIS_FOLDER + '/' + RUNID + '-05-assemble\n')
  f.write('consensus_path: ' + ANALYSIS_FOLDER + '/' + RUNID + '-06-consensus\n')
  f.write('results: ' + ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS\n')
  f.write('benchmark: ' + ANALYSIS_FOLDER + '/' + RUNID + '_BENCHMARK\n')
  f.write('barcode_kit: ' + (' ').join(inputArgs.barcodeKit)+ '\n')
  f.write('db: ' + inputArgs.db + '\n')
  f.write('target: ' + (',').join(inputArgs.targetPathogen) + '\n')
  f.write('script_path: ' + scriptPath + '\n')
  f.write('assembler: ' + (',').join(inputArgs.assembler) + '\n')
  f.write('genomeSize: ' + str(inputArgs.genomeSize) + '\n')
  f.write('covLimit: '+ str(inputArgs.covLimit)+ '\n')
  if (inputArgs.manualRef):
    f.write('manual_ref: ' + (',').join(inputArgs.manualRef)+ '\n')
  f.write('label: ' + inputArgs.label + '\n')
  f.write('runidwolabel: ' + inputArgs.runid + '\n')
  f.write('CompletionDay: '+ str(nowTimeDay)+ '\n')
  f.write('CommonViruses: '+ scriptPath +'/common-viruses.txt\n')


logFormatter = logging.Formatter('%(levelname)s\t%(asctime)s\t%(message)s', datefmt='%I:%M:%S %d.%m.%Y')
logger = logging.getLogger()

os.makedirs(ANALYSIS_FOLDER + '/'+RUNID+'_RESULTS/logfiles', exist_ok=True)
analysis_log_file = ANALYSIS_FOLDER+'/'+RUNID+'_RESULTS/logfiles/analysis-'+nowTime+'.log'
fileHandler = logging.FileHandler(analysis_log_file)
fileHandler.setFormatter(logFormatter)
logger.addHandler(fileHandler)

consoleHandler = logging.StreamHandler(sys.stdout)
consoleHandler.setFormatter(logFormatter)
logger.addHandler(consoleHandler)
logger.setLevel(logging.DEBUG)

###################################################################################################################################################################

logger.info("Starting nanoflow")
logger.info('nanoflow version: '+analysis_log_file)
get_git_tag = 'git -C '+scriptPath+' describe HEAD --always'
with subprocess.Popen(get_git_tag, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell = True,bufsize=1, universal_newlines=True) as p:
    for line in p.stdout:
        logger.info(line.rstrip())
        with open(ANALYSIS_FOLDER+'/config.yaml', 'a+') as f:
            f.write('version: '+ line.split('-')[0] + '\n')
logger.info('Analysis log: '+analysis_log_file)
logger.info('Arguments given:')
logger.info(inputArgs)
logger.info('Parameter file:')
with open(ANALYSIS_FOLDER+'/config.yaml', 'r') as f:
  lines = f.readlines() 
  for line in lines:
    logger.info(''.join(line.split()))
logger.info('General environment file:')
with open(scriptPath + '/envs/general.yaml', 'r') as f:
  lines = f.readlines() 
  for line in lines:
    logger.info(''.join(line.split()))


###################################################################################################################################################################

if (not inputArgs.demultiplex and not inputArgs.trim and not inputArgs.clean 
  and not inputArgs.map and not inputArgs.assemble 
  and not inputArgs.consensus and not inputArgs.keep_bam):
  logger.info('Running complete pipeline')
  All = True

###################################################################################################################################################################

if (inputArgs.demultiplex):

  logger.info('Demultiplex')
  demultiplex_log_file = ANALYSIS_FOLDER+'/'+RUNID+'_RESULTS/logfiles/demultiplex-'+nowTime+'.log'
  logger.info('Demupltiplex log file: ' + demultiplex_log_file)
  fileHandler = logging.FileHandler(demultiplex_log_file)
  fileHandler.setFormatter(logFormatter)
  logger.addHandler(fileHandler)
  fastqdirs = []
  for directory in inputArgs.fastqDir:
    fastqdirs.append(list(set(glob.glob(directory +'/*/*/fastq_*', recursive=True)) - set(glob.glob(directory +'/*/*/fastq_fail'))))
  fastqdirs = [item for sublist in fastqdirs for item in sublist]
  for fastqdir in fastqdirs:
    print(fastqdir)
    guppy_command = 'guppy_barcoder -r -i ' + fastqdir + ' -x auto -s '+ ANALYSIS_FOLDER + '/' + RUNID + '-01-demultiplex/' + ('-').join(fastqdir.split('/')[-2:])+ '  --compress_fastq --trim_barcodes --require_barcodes_both_ends --detect_mid_strand_barcodes --barcode_kits "'+ (' ').join(inputArgs.barcodeKit)+ '"'
    with subprocess.Popen(guppy_command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell = True,bufsize=1, universal_newlines=True) as p:
        for line in p.stdout:
            logger.info(line.rstrip()) 

  for fastqdir in fastqdirs:
    print(fastqdir)
    filenamesBC = glob.glob(ANALYSIS_FOLDER + '/' + RUNID + '-01-demultiplex/' + ('-').join(fastqdir.split('/')[-2:]) +'/*barcode*')
    for BC in filenamesBC:
      barcodetmp = BC.split('/')[-1]
      print(barcodetmp)

      with open(ANALYSIS_FOLDER + '/' + RUNID + '-01-demultiplex/'+RUNID+'-'+barcodetmp+'-demultiplexed.fastq', 'a') as outfile:
        for fname in glob.glob(BC+'/*.fastq*'):
            if '.fastq.gz' in fname:
              with gzip.open(fname, mode = 'rt') as infile:
                for line in infile:
                  outfile.write(line)
            else:
              with open(fname) as infile:
                for line in infile:
                  outfile.write(line)

  fileHandler = logging.FileHandler(analysis_log_file)
  fileHandler.setFormatter(logFormatter)
  logger.addHandler(fileHandler)

  logger.info('Demultiplexing finished')

###################################################################################################################################################################

logger.info('Samples to run:')
if (inputArgs.selectedSamples):
  samplesToRun = inputArgs.selectedSamples
else:
  if not inputArgs.norm_fastq:
    if not glob.glob(ANALYSIS_FOLDER + '/' + RUNID + '-01-demultiplex/'+RUNID+'-barcode*-demultiplexed.fastq*'):
      fastqdirs = []
      for directory in inputArgs.fastqDir:
        fastqdirs.append(list(set(glob.glob(directory +'/**/*pass/barcode*', recursive=True)) - set(glob.glob(directory +'/**/*fail/barcode*', recursive=True))- set(glob.glob(directory +'/**/*fast5*/barcode*', recursive=True))))
      fastqdirs = [item for sublist in fastqdirs for item in sublist]
      for BC in fastqdirs:
        logger.info('Barcode folder: '+BC)
        barcodetmp = BC.split('/')[-1]
        os.makedirs(ANALYSIS_FOLDER + '/' + RUNID + '-01-demultiplex/',exist_ok=True)
        with open(ANALYSIS_FOLDER + '/' + RUNID + '-01-demultiplex/'+RUNID+'-'+barcodetmp+'-demultiplexed.fastq', 'a') as outfile:
          for fname in glob.glob(BC+'/*.fastq*'):
              # print(fname)
              with open(fname) as infile:
                  for line in infile:
                      outfile.write(line)

    listFullDir = [i.split('/')[-1] for i in glob.glob(ANALYSIS_FOLDER + '/' + RUNID + '-01-demultiplex/'+RUNID+'-barcode*-demultiplexed.fastq*')]
    samplesToRunTmp = [re.search(RUNID+'-(.*)-demultiplexed.fastq', i) for i in listFullDir]
    samplesToRun = [j.group(1) for j in samplesToRunTmp]

  else:
    samplesToRun = []
    for i in inputArgs.norm_fastq:
      if 'reads' in i:
        sampleToRunTmp = i.split('-normalised-reads-with')[0].split('/')[-1]
        sampleToRunTmp = 'reads-'+ sampleToRunTmp 
      if 'bases' in i:
        sampleToRunTmp = i.split('-normalised-bases-with')[0].split('/')[-1]
        sampleToRunTmp = 'reads-'+ sampleToRunTmp 

      os.makedirs(ANALYSIS_FOLDER + '/' + RUNID + '-02-trim',exist_ok=True)
      shutil.copyfile(i, ANALYSIS_FOLDER + '/' + RUNID + '-02-trim/'+RUNID+'-'+sampleToRunTmp+'-seqtk-trimfq.fastq')
      samplesToRun.append(sampleToRunTmp)
with open(ANALYSIS_FOLDER+'/table.csv', 'w+') as f:
  for item in sorted(samplesToRun):
    f.write('%s\n' % item)
    logger.info(item)


###################################################################################################################################################################

if (All or inputArgs.trim):

  trim_log_file = ANALYSIS_FOLDER+'/'+RUNID+'_RESULTS/logfiles/trim-'+nowTime+'.log'
  logger.info('Trim')
  logger.info('Trim log file: '+trim_log_file)
  fileHandler = logging.FileHandler(trim_log_file)
  fileHandler.setFormatter(logFormatter)
  logger.addHandler(fileHandler)
  trim_command = 'snakemake --snakefile '+scriptPath+'/02.trim.snakefile \
  --configfile '+ANALYSIS_FOLDER+'/config.yaml --use-conda --conda-prefix '+home+'/opt/iflow \
  --cores '+ str(inputArgs.threads) 

  with subprocess.Popen(trim_command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell = True,bufsize=1, universal_newlines=True) as p:
      for line in p.stdout:
          logger.info(line.rstrip()) 

if (All or inputArgs.trim or inputArgs.classify):
  logger.info("Centrifuge environment file:")
  with open(scriptPath + '/envs/centrifuge.yaml', 'r') as f:
    lines = f.readlines() 
    for line in lines:
      logger.info(''.join(line.split()))  
  trim_classify_command = 'snakemake --snakefile '+scriptPath+'/ana/classification.snakefile \
  --configfile '+ANALYSIS_FOLDER+'/config.yaml --use-conda --conda-prefix '+home+'/opt/iflow \
  --cores  '+ str(inputArgs.threads)
  with subprocess.Popen(trim_classify_command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell = True, bufsize=1, universal_newlines=True) as p:
      for line in p.stdout:
          logger.info(line.rstrip()) 

  fileHandler = logging.FileHandler(analysis_log_file)
  fileHandler.setFormatter(logFormatter)
  logger.addHandler(fileHandler)
  logger.info('Trim finished')

###################################################################################################################################################################

if (All or inputArgs.clean):

  clean_log_file = ANALYSIS_FOLDER+'/'+RUNID+'_RESULTS/logfiles/clean-'+nowTime+'.log'
  logger.info('Clean for contaminants: ' + cleanOpts)
  logger.info('Clean log file: '+clean_log_file)
  fileHandler = logging.FileHandler(clean_log_file)
  fileHandler.setFormatter(logFormatter)
  logger.addHandler(fileHandler)
  clean_script_command = 'python '+scriptPath+'/03.clean.py ' + ANALYSIS_FOLDER + ' \
  '+ cleanOpts + ' '+ RUNID + ' ' +ANALYSIS_FOLDER + '/' + RUNID + '-02-trim \
  ' +' ' +  ANALYSIS_FOLDER + '/' + RUNID + '-03-clean '+ scriptPath + ' ' + str(inputArgs.threads)

  with subprocess.Popen(clean_script_command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell = True,bufsize=1, universal_newlines=True) as p:
      for line in p.stdout:
          logger.info(line.rstrip())

  fileHandler = logging.FileHandler(analysis_log_file)
  fileHandler.setFormatter(logFormatter)
  logger.addHandler(fileHandler)
  logger.info('Clean finished')

  ###################################################################################################################################################################

if (All or inputArgs.map):

  map_log_file = ANALYSIS_FOLDER+'/'+RUNID+'_RESULTS/logfiles/map-'+nowTime+'.log'
  logger.info('Mapping to: ' + (',').join(inputArgs.targetPathogen))
  logger.info('Map log file: '+map_log_file)
  fileHandler = logging.FileHandler(map_log_file)
  fileHandler.setFormatter(logFormatter)
  logger.addHandler(fileHandler)

  logger.info('Map: pre-assembly')
  map_scriptA_command = 'snakemake --snakefile '+scriptPath+'/04.mapA-preassembly.snakefile \
  --configfile '+ANALYSIS_FOLDER+'/config.yaml --use-conda --conda-prefix '+home+'/opt/iflow \
  --cores '+ str(inputArgs.threads) 

  with subprocess.Popen(map_scriptA_command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell = True,bufsize=1, universal_newlines=True) as p:
    for line in p.stdout:
        logger.info(line.rstrip())


  if ('canu' in inputArgs.assembler):
    logger.info("Canu environment file:")

    with open(scriptPath + '/envs/canu.yaml', 'r') as f:
      lines = f.readlines() 
      for line in lines:
        logger.info(''.join(line.split()))

    df = pd.read_csv(ANALYSIS_FOLDER + '/table.csv', names = ['sample'])
    stats = pd.DataFrame(columns=['num_seqs', 'sum_len', 'min_len', 'avg_len','max_len'])

    SAMPLES = df['sample']
    TARGETS = (',').join(inputArgs.targetPathogen).split(',')
    for sample in SAMPLES:
      for target in TARGETS:
        logger.info(sample+ ': Assembly of '+target+' mapped reads with canu')
        os.makedirs(ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/'+sample+'/03_map-'+target,exist_ok=True)
        canuAttemps_file = ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/'+sample+'/03_map-'+target+'/canu-attempts.tsv'
        Path(canuAttemps_file).touch()
        if (os.path.exists(canuAttemps_file) and os.path.getsize(canuAttemps_file) > 0):
          canuAttempts = pd.read_csv(canuAttemps_file, names = ['Attempt number','Status'],sep="\t")
        else:
          data = {'Attempt number':[0], 'Status':['failed']}
          canuAttempts = pd.DataFrame(data)

        while (canuAttempts['Status'].iloc[-1] != 'success' and not canuAttempts['Attempt number'].iloc[-1] == 4):
          lastAttempt = int(canuAttempts['Attempt number'].iloc[-1])
          lastAttemptStatus = canuAttempts['Status'].iloc[-1]
          lastAttempt += 1

          if (lastAttempt == 1):
            logger.info('Running attempt number 1')

            minReadLength = 300
            minOverlapLength = 200
            subsampleLevel = 75000
            logger.info('First canu attempt')
            canu_command = 'snakemake --snakefile '+scriptPath+'/04.mapB-assembly-canu.snakefile \
            --configfile '+ANALYSIS_FOLDER+'/config.yaml --config currentSample='+sample+' currentTarget='+target+' attemptNumber='+str(lastAttempt)+' minReadLength='+str(minReadLength)+' minOverlapLength='+str(minOverlapLength)+' subsampleLevel='+str(subsampleLevel)+' --use-conda --conda-prefix '+home+'/opt/iflow \
            --cores '+ str(inputArgs.threads)
            with subprocess.Popen(canu_command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell = True,bufsize=1, universal_newlines=True) as p:
              for line in p.stdout:
                logger.info(line.rstrip())

            contigsFile = ANALYSIS_FOLDER + '/' + RUNID + '-04-map' + '/'+sample+'-'+target+'/02_assemble-canu-'+str(lastAttempt)+'/'+RUNID+'-'+sample+'-'+target+'.contigs.fasta'
            if os.path.exists(contigsFile) and os.path.getsize(contigsFile) > 0:
              shutil.copyfile(contigsFile, ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/'+sample+'/03_map-'+target+'/'+RUNID+'-'+sample+'-'+target+'-canu-contigs.fasta')
              data = {'Attempt number':lastAttempt, 'Status':['success']}
              tmp = pd.DataFrame(data)
              canuAttempts = canuAttempts.append(tmp,sort=True)
              canufile = open(canuAttemps_file, 'w+')
              canufile.write(str(lastAttempt)+'\tsuccess')
              canufile.close()
            else:
              data = {'Attempt number':lastAttempt, 'Status':['failed']}
              tmp = pd.DataFrame(data)
              canuAttempts = canuAttempts.append(tmp,sort=True)
              canufile = open(canuAttemps_file, 'w+')
              canufile.write(str(lastAttempt)+'\tfailed')
              canufile.close()

          if (lastAttempt == 2):
            logger.info('Running attempt number 2')

            minReadLength = 200
            minOverlapLength = 50
            subsampleLevel = 75000
            logger.info('Second canu attempt')
            canu_command = 'snakemake --snakefile '+scriptPath+'/04.mapB-assembly-canu.snakefile \
            --configfile '+ANALYSIS_FOLDER+'/config.yaml --config currentSample='+sample+' currentTarget='+target+' attemptNumber='+str(lastAttempt)+' minReadLength='+str(minReadLength)+' minOverlapLength='+str(minOverlapLength)+' subsampleLevel='+str(subsampleLevel)+' --use-conda --conda-prefix '+home+'/opt/iflow \
            --cores '+ str(inputArgs.threads)
            with subprocess.Popen(canu_command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell = True,bufsize=1, universal_newlines=True) as p:
              for line in p.stdout:
                logger.info(line.rstrip())
            
            contigsFile = ANALYSIS_FOLDER + '/' + RUNID + '-04-map' + '/'+sample+'-'+target+'/02_assemble-canu-'+str(lastAttempt)+'/'+RUNID+'-'+sample+'-'+target+'.contigs.fasta'
            if os.path.exists(contigsFile) and os.path.getsize(contigsFile) > 0:
              shutil.copyfile(contigsFile, ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/'+sample+'/03_map-'+target+'/'+RUNID+'-'+sample+'-'+target+'-canu-contigs.fasta')
              data = {'Attempt number':lastAttempt, 'Status':['success']}
              tmp = pd.DataFrame(data)
              canuAttempts = canuAttempts.append(tmp,sort=True)
              canufile = open(canuAttemps_file, 'w+')
              canufile.write(str(lastAttempt)+'\tsuccess')
              canufile.close()
            else:
              data = {'Attempt number':lastAttempt, 'Status':['failed']}
              tmp = pd.DataFrame(data)
              canuAttempts = canuAttempts.append(tmp,sort=True)
              canufile = open(canuAttemps_file, 'w+')
              canufile.write(str(lastAttempt)+'\tfailed')
              canufile.close()


          if (lastAttempt == 3):
            logger.info('Running attempt number 3')

            minReadLength = 300
            minOverlapLength = 200
            subsampleLevel = 50000
            logger.info('Third canu attempt')
            canu_command = 'snakemake --snakefile '+scriptPath+'/04.mapB-assembly-canu.snakefile \
            --configfile '+ANALYSIS_FOLDER+'/config.yaml --config currentSample='+sample+' currentTarget='+target+' attemptNumber='+str(lastAttempt)+' minReadLength='+str(minReadLength)+' minOverlapLength='+str(minOverlapLength)+' subsampleLevel='+str(subsampleLevel)+' --use-conda --conda-prefix '+home+'/opt/iflow \
            --cores '+ str(inputArgs.threads)
            with subprocess.Popen(canu_command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell = True,bufsize=1, universal_newlines=True) as p:
              for line in p.stdout:
                logger.info(line.rstrip())

            contigsFile = ANALYSIS_FOLDER + '/' + RUNID + '-04-map' + '/'+sample+'-'+target+'/02_assemble-canu-'+str(lastAttempt)+'/'+RUNID+'-'+sample+'-'+target+'.contigs.fasta'
            if os.path.exists(contigsFile) and os.path.getsize(contigsFile) > 0:
              shutil.copyfile(contigsFile, ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/'+sample+'/03_map-'+target+'/'+RUNID+'-'+sample+'-'+target+'-canu-contigs.fasta')
              data = {'Attempt number':lastAttempt, 'Status':['success']}
              tmp = pd.DataFrame(data)
              canuAttempts = canuAttempts.append(tmp,sort=True)
              canufile = open(canuAttemps_file, 'w+')
              canufile.write(str(lastAttempt)+'\tsuccess')
              canufile.close()
            else:
              data = {'Attempt number':lastAttempt, 'Status':['failed']}
              tmp = pd.DataFrame(data)
              canuAttempts = canuAttempts.append(tmp,sort=True)
              canufile = open(canuAttemps_file, 'w+')
              canufile.write(str(lastAttempt)+'\tfailed')
              canufile.close()


          if (lastAttempt == 4):
            logger.info('Running attempt number 4')

            minReadLength = 300
            minOverlapLength = 200
            subsampleLevel = 50000
            logger.info('Forth canu attempt')
            canu_command = 'snakemake --snakefile '+scriptPath+'/04.mapB-assembly-canu.snakefile \
            --configfile '+ANALYSIS_FOLDER+'/config.yaml --config currentSample='+sample+' currentTarget='+target+' attemptNumber='+str(lastAttempt)+' minReadLength='+str(minReadLength)+' minOverlapLength='+str(minOverlapLength)+' subsampleLevel='+str(subsampleLevel)+' --use-conda --conda-prefix '+home+'/opt/iflow \
            --cores '+ str(inputArgs.threads)
            with subprocess.Popen(canu_command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell = True,bufsize=1, universal_newlines=True) as p:
              for line in p.stdout:
                logger.info(line.rstrip())

            contigsFile = ANALYSIS_FOLDER + '/' + RUNID + '-04-map' + '/'+sample+'-'+target+'/02_assemble-canu-'+str(lastAttempt)+'/'+RUNID+'-'+sample+'-'+target+'.contigs.fasta'
            if os.path.exists(contigsFile) and os.path.getsize(contigsFile) > 0:
              shutil.copyfile(contigsFile, ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/'+sample+'/03_map-'+target+'/'+RUNID+'-'+sample+'-'+target+'-canu-contigs.fasta')
              data = {'Attempt number':lastAttempt, 'Status':['success']}
              tmp = pd.DataFrame(data)
              canuAttempts = canuAttempts.append(tmp,sort=True)
              canufile = open(canuAttemps_file, 'w+')
              canufile.write(str(lastAttempt)+'\tsuccess')
              canufile.close()
            else:
              data = {'Attempt number':lastAttempt, 'Status':['failed']}
              tmp = pd.DataFrame(data)
              canuAttempts = canuAttempts.append(tmp,sort=True)
              canufile = open(canuAttemps_file, 'w+')
              canufile.write(str(lastAttempt)+'\tfailed')
              canufile.close()
              Path('contigsFile').touch()

              with open(ANALYSIS_FOLDER + '/' + RUNID + '-04-map' + '/' + sample + '-' + target + '/' + '01_map/' + RUNID+ '-' + sample + '-' + target + '-' + 'subsampled-50000.fastq','r') as input_handle:
                with open(contigsFile, "w") as output_handle:
                  sequences = SeqIO.parse(input_handle, "fastq")
                  count = SeqIO.write(sequences, output_handle, "fasta")

            shutil.copyfile(contigsFile, ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/'+sample+'/03_map-'+target+'/'+RUNID+'-'+sample+'-'+target+'-canu-contigs.fasta')

            Path(ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/'+sample+'/03_map-'+target+'/'+RUNID+'-'+sample+'-'+target+'-canu-contigs.fasta').touch()

      logger.info('Finishing attempt: '+str(canuAttempts['Attempt number'].iloc[-1]))

    logger.info('Map: post-assembly')

    ## Execute only if mapped file is
    map_scriptC_command = 'snakemake --snakefile '+scriptPath+'/04.mapC-postassembly.snakefile \
    --configfile '+ANALYSIS_FOLDER+'/config.yaml --use-conda --conda-prefix '+home+'/opt/iflow \
    --cores '+ str(inputArgs.threads) 

    with subprocess.Popen(map_scriptC_command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell = True,bufsize=1, universal_newlines=True) as p:
      for line in p.stdout:
          logger.info(line.rstrip())

    df = pd.read_csv(ANALYSIS_FOLDER + '/table.csv', names = ['sample'])
    SAMPLES = df['sample']
    TARGETS = (',').join(inputArgs.targetPathogen).split(',')
    for sample in SAMPLES:
      stats_mapped = pd.DataFrame(columns=['num_seqs', 'sum_len', 'min_len', 'avg_len','max_len','target'])
      stats_mapped_contigs = pd.DataFrame(columns=['num_seqs', 'sum_len', 'min_len', 'avg_len','max_len','target'])

      for target in TARGETS:
        if os.path.exists(ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/' + sample +'/03_map-'+ target +  '/'+RUNID+'-'+sample+'-'+target+'-mapped-stats.txt') and os.path.getsize(ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/' + sample +'/03_map-'+ target +  '/'+RUNID+'-'+sample+'-'+target+'-mapped-stats.txt') > 0:
          stat_file = open(ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/' + sample +'/03_map-'+ target +  '/'+RUNID+'-'+sample+'-'+target+'-mapped-stats.txt','r')
          lines = stat_file.readlines()
          tmp = pd.Series((re.sub(',','',lines[1])).split()[-5], index=['num_seqs', 'sum_len', 'min_len', 'avg_len','max_len'])
          tmp['target']= target
          stats_mapped = stats_mapped.append(tmp, ignore_index=True)
          stat_file.close()
        #Statistics on the mapped contigs
        if os.path.exists(ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/' + sample +'/03_map-'+ target +  '/'+RUNID+'-'+sample+'-'+target+'-canu-mapped-assembly-stats.txt') and os.path.getsize(ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/' + sample +'/03_map-'+ target +  '/'+RUNID+'-'+sample+'-'+target+'-canu-mapped-assembly-stats.txt') > 0:
          stat_file = open(ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/' + sample +'/03_map-'+ target +  '/'+RUNID+'-'+sample+'-'+target+'-canu-mapped-assembly-stats.txt','r')
          lines = stat_file.readlines()
          tmp = pd.Series((re.sub(',','',lines[1])).split()[-5], index=['num_seqs', 'sum_len', 'min_len', 'avg_len','max_len'])
          tmp['target']= target
          stats_mapped_contigs = stats_mapped_contigs.append(tmp, ignore_index=True)
          stat_file.close()

      if os.path.exists(ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/'+sample+'/03_map-' + RUNID+'-'+sample+'-mapped-stats-all-targets.txt'):
        os.remove(ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/'+sample+'/03_map-' + RUNID+'-'+sample+'-mapped-stats-all-targets.txt')
      stats_mapped.to_csv(ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/'+sample+'/03_map-' + RUNID+'-'+sample+'-mapped-stats-all-targets.txt', index=False)
      if os.path.exists(ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/'+sample+'/03_map-' + RUNID+'-'+sample+'-canu-mapped-assembly-stats-all-targets.txt'):
        os.remove(ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/'+sample+'/03_map-' + RUNID+'-'+sample+'-canu-mapped-assembly-stats-all-targets.txt')
      stats_mapped_contigs.to_csv(ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/'+sample+'/03_map-' + RUNID+'-'+sample+'-canu-mapped-assembly-stats-all-targets.txt', index=False)

  fileHandler = logging.FileHandler(analysis_log_file)
  fileHandler.setFormatter(logFormatter)
  logger.addHandler(fileHandler)
  logger.info('Mapping finished')


  ###################################################################################################################################################################

if (All or inputArgs.assemble):

  assemble_log_file = ANALYSIS_FOLDER+'/'+RUNID+'_RESULTS/logfiles/assemble-'+nowTime+'.log'
  logger.info('Assemble')
  logger.info('Assemble log file: '+assemble_log_file)
  fileHandler = logging.FileHandler(assemble_log_file)
  fileHandler.setFormatter(logFormatter)
  logger.addHandler(fileHandler)

  logger.info('Assemble: pre-assembly')
  assemble_scriptA_command = 'snakemake --snakefile '+scriptPath+'/05.assembleA-preassembly.snakefile \
  --configfile '+ANALYSIS_FOLDER+'/config.yaml --use-conda --conda-prefix '+home+'/opt/iflow \
  --cores '+ str(inputArgs.threads) 

  with subprocess.Popen(assemble_scriptA_command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell = True,bufsize=1, universal_newlines=True) as p:
    for line in p.stdout:
        logger.info(line.rstrip())

  if ('canu' in inputArgs.assembler):
    logger.info("Canu environment file:")

    with open(scriptPath + '/envs/canu.yaml', 'r') as f:
      lines = f.readlines() 
      for line in lines:
        logger.info(''.join(line.split()))
    df = pd.read_csv(ANALYSIS_FOLDER + '/table.csv', names = ['sample'])
    SAMPLES = df['sample']
    TARGETS = (',').join(inputArgs.targetPathogen).split(',')
    for sample in SAMPLES:
      logger.info(sample+ ': Assembly with canu')
      os.makedirs(ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/'+sample+'/04_assemble',exist_ok=True)
      canuAttemps_file = ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/'+sample+'/04_assemble/canu-attempts.tsv'
      Path(canuAttemps_file).touch()
      if (os.path.exists(canuAttemps_file) and os.path.getsize(canuAttemps_file) > 0):
        canuAttempts = pd.read_csv(canuAttemps_file, names = ['Attempt number','Status'],sep="\t")
      else:
        data = {'Attempt number':[0], 'Status':['failed']}
        canuAttempts = pd.DataFrame(data)

      while (canuAttempts['Status'].iloc[-1] != 'success' and not canuAttempts['Attempt number'].iloc[-1] == 4):

        lastAttempt = int(canuAttempts['Attempt number'].iloc[-1])
        lastAttemptStatus = canuAttempts['Status'].iloc[-1]
        lastAttempt += 1

        if (lastAttempt == 1):
          logger.info('Running attempt number 1')

          minReadLength = 300
          minOverlapLength = 200
          subsampleLevel = 75000
          logger.info('First canu attempt')
          canu_command = 'snakemake --snakefile '+scriptPath+'/05.assembleB-assembly-canu.snakefile \
          --configfile '+ANALYSIS_FOLDER+'/config.yaml --config currentSample='+sample+' attemptNumber='+str(lastAttempt)+' minReadLength='+str(minReadLength)+' minOverlapLength='+str(minOverlapLength)+' subsampleLevel='+str(subsampleLevel)+' --use-conda --conda-prefix '+home+'/opt/iflow \
          --cores '+ str(inputArgs.threads)
          with subprocess.Popen(canu_command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell = True,bufsize=1, universal_newlines=True) as p:
            for line in p.stdout:
              logger.info(line.rstrip())

          contigsFile = ANALYSIS_FOLDER + '/' + RUNID + '-05-assemble' + '/'+sample+'/02_assemble-canu-'+str(lastAttempt)+'/' + RUNID+'-'+sample+'.contigs.fasta'
          if os.path.exists(contigsFile) and os.path.getsize(contigsFile) > 0:
            shutil.copyfile(contigsFile, ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/'+sample+'/04_assemble/'+RUNID+'-'+sample+'-'+'canu-contigs.fasta')
            data = {'Attempt number':lastAttempt, 'Status':['success']}
            tmp = pd.DataFrame(data)
            canuAttempts = canuAttempts.append(tmp,sort=True)
            canufile = open(canuAttemps_file, 'w+')
            canufile.write(str(lastAttempt)+'\tsuccess')
            canufile.close()
          else:
            data = {'Attempt number':lastAttempt, 'Status':['failed']}
            tmp = pd.DataFrame(data)
            canuAttempts = canuAttempts.append(tmp,sort=True)
            canufile = open(canuAttemps_file, 'w+')
            canufile.write(str(lastAttempt)+'\tfailed')
            canufile.close()

        if (lastAttempt == 2):
          logger.info('Running attempt number 2')

          minReadLength = 200
          minOverlapLength = 50
          subsampleLevel = 75000
          logger.info('Second canu attempt')
          canu_command = 'snakemake --snakefile '+scriptPath+'/05.assembleB-assembly-canu.snakefile \
          --configfile '+ANALYSIS_FOLDER+'/config.yaml --config currentSample='+sample+' attemptNumber='+str(lastAttempt)+' minReadLength='+str(minReadLength)+' minOverlapLength='+str(minOverlapLength)+' subsampleLevel='+str(subsampleLevel)+' --use-conda --conda-prefix '+home+'/opt/iflow \
          --cores '+ str(inputArgs.threads)
          with subprocess.Popen(canu_command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell = True,bufsize=1, universal_newlines=True) as p:
            for line in p.stdout:
              logger.info(line.rstrip())

          contigsFile = ANALYSIS_FOLDER + '/' + RUNID + '-05-assemble' + '/'+sample+'/02_assemble-canu-'+str(lastAttempt)+'/' + RUNID+'-'+sample+'.contigs.fasta'
          if os.path.exists(contigsFile) and os.path.getsize(contigsFile) > 0:
            shutil.copyfile(contigsFile, ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/'+sample+'/04_assemble/'+RUNID+'-'+sample+'-'+'canu-contigs.fasta')
            data = {'Attempt number':lastAttempt, 'Status':['success']}
            tmp = pd.DataFrame(data)
            canuAttempts = canuAttempts.append(tmp,sort=True)
            canufile = open(canuAttemps_file, 'w+')
            canufile.write(str(lastAttempt)+'\tsuccess')
            canufile.close()
          else:
            data = {'Attempt number':lastAttempt, 'Status':['failed']}
            tmp = pd.DataFrame(data)
            canuAttempts = canuAttempts.append(tmp,sort=True)
            canufile = open(canuAttemps_file, 'w+')
            canufile.write(str(lastAttempt)+'\tfailed')
            canufile.close()

        if (lastAttempt == 3):
          logger.info('Running attempt number 3')

          minReadLength = 300
          minOverlapLength = 200
          subsampleLevel = 50000
          logger.info('Third canu attempt')
          canu_command = 'snakemake --snakefile '+scriptPath+'/05.assembleB-assembly-canu.snakefile \
          --configfile '+ANALYSIS_FOLDER+'/config.yaml --config currentSample='+sample+' attemptNumber='+str(lastAttempt)+' minReadLength='+str(minReadLength)+' minOverlapLength='+str(minOverlapLength)+' subsampleLevel='+str(subsampleLevel)+' --use-conda --conda-prefix '+home+'/opt/iflow \
          --cores '+ str(inputArgs.threads)
          with subprocess.Popen(canu_command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell = True,bufsize=1, universal_newlines=True) as p:
            for line in p.stdout:
              logger.info(line.rstrip())

          contigsFile = ANALYSIS_FOLDER + '/' + RUNID + '-05-assemble' + '/'+sample+'/02_assemble-canu-'+str(lastAttempt)+'/' + RUNID+'-'+sample+'.contigs.fasta'
          if os.path.exists(contigsFile) and os.path.getsize(contigsFile) > 0:
            shutil.copyfile(contigsFile, ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/'+sample+'/04_assemble/'+RUNID+'-'+sample+'-'+'canu-contigs.fasta')
            data = {'Attempt number':lastAttempt, 'Status':['success']}
            tmp = pd.DataFrame(data)
            canuAttempts = canuAttempts.append(tmp,sort=True)
            canufile = open(canuAttemps_file, 'w+')
            canufile.write(str(lastAttempt)+'\tsuccess')
            canufile.close()
          else:
            data = {'Attempt number':lastAttempt, 'Status':['failed']}
            tmp = pd.DataFrame(data)
            canuAttempts = canuAttempts.append(tmp,sort=True)
            canufile = open(canuAttemps_file, 'w+')
            canufile.write(str(lastAttempt)+'\tfailed')
            canufile.close()

        if (lastAttempt == 4):
          logger.info('Running attempt number 4')

          minReadLength = 300
          minOverlapLength = 200
          subsampleLevel = 50000
          logger.info('Forth canu attempt')
          canu_command = 'snakemake --snakefile '+scriptPath+'/05.assembleB-assembly-canu.snakefile \
          --configfile '+ANALYSIS_FOLDER+'/config.yaml --config currentSample='+sample+' attemptNumber='+str(lastAttempt)+' minReadLength='+str(minReadLength)+' minOverlapLength='+str(minOverlapLength)+' subsampleLevel='+str(subsampleLevel)+' --use-conda --conda-prefix '+home+'/opt/iflow \
          --cores '+ str(inputArgs.threads)
          with subprocess.Popen(canu_command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell = True,bufsize=1, universal_newlines=True) as p:
            for line in p.stdout:
              logger.info(line.rstrip())

          contigsFile = ANALYSIS_FOLDER + '/' + RUNID + '-05-assemble' + '/'+sample+'/02_assemble-canu-'+str(lastAttempt)+'/' + RUNID+'-'+sample+'.contigs.fasta'
          if os.path.exists(contigsFile) and os.path.getsize(contigsFile) > 0:
            shutil.copyfile(contigsFile, ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/'+sample+'/04_assemble/'+RUNID+'-'+sample+'-'+'canu-contigs.fasta')
            data = {'Attempt number':lastAttempt, 'Status':['success']}
            tmp = pd.DataFrame(data)
            canuAttempts = canuAttempts.append(tmp,sort=True)
            canufile = open(canuAttemps_file, 'w+')
            canufile.write(str(lastAttempt)+'\tsuccess')
            canufile.close()
          else:
            data = {'Attempt number':lastAttempt, 'Status':['failed']}
            tmp = pd.DataFrame(data)
            canuAttempts = canuAttempts.append(tmp,sort=True)
            canufile = open(canuAttemps_file, 'w+')
            canufile.write(str(lastAttempt)+'\tfailed')
            canufile.close()
            Path(ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/'+sample+'/04_assemble/'+RUNID+'-'+sample+'-'+'canu-contigs.fasta').touch()

    logger.info('Assemble: post-assembly')
    assemble_scriptC_command = 'snakemake --snakefile '+scriptPath+'/05.assembleC-postassembly.snakefile \
    --configfile '+ANALYSIS_FOLDER+'/config.yaml --use-conda --conda-prefix '+home+'/opt/iflow \
    --cores '+ str(inputArgs.threads) 

    with subprocess.Popen(assemble_scriptC_command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell = True,bufsize=1, universal_newlines=True) as p:
      for line in p.stdout:
          logger.info(line.rstrip())

  fileHandler = logging.FileHandler(analysis_log_file)
  fileHandler.setFormatter(logFormatter)
  logger.addHandler(fileHandler)
  logger.info('Assembly finished')

###################################################################################################################################################################

if (All or inputArgs.consensus):

  consensus_log_file = ANALYSIS_FOLDER+'/'+RUNID+'_RESULTS/logfiles/consensus-'+nowTime+'.log'
  logger.info('Consensus generation')
  logger.info('Consensus log file: '+consensus_log_file)
  fileHandler = logging.FileHandler(consensus_log_file)
  fileHandler.setFormatter(logFormatter)
  logger.addHandler(fileHandler)


  df = pd.read_csv(ANALYSIS_FOLDER + '/table.csv', names = ['sample'])
  SAMPLES = df['sample']
  TARGETS = (',').join(inputArgs.targetPathogen).split(',')
  ASSEMBLERS = (',').join(inputArgs.assembler).split(',')
  blastedList = pd.DataFrame()

  for sample in SAMPLES:
    for assembler in ASSEMBLERS:
      for target in TARGETS:
        mappedList = ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/'+sample+'/03_map-'+target+'/'+RUNID+'-'+sample+'-'+target+'-'+assembler+'-blasted-list.csv'
        if os.path.exists(mappedList) and os.path.getsize(mappedList) > 0:
          tmp = pd.read_csv(mappedList, names = ['sample','ref','def','length','target','assembler','bitscore'])
          blastedList = blastedList.append(tmp[['sample','ref','target']])


      assembledList = ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/'+sample+'/04_assemble/'+RUNID+'-'+sample+'-'+assembler+'-blasted-list.csv'
      if os.path.exists(assembledList) and os.path.getsize(assembledList) > 0:
        tmp = pd.read_csv(assembledList, names = ['sample','ref','def','length','target','bitscore'])
        blastedList = blastedList.append(tmp[['sample','ref','target']])
    os.makedirs(ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/'+sample,exist_ok=True)
    manualList = ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/'+sample+'/'+RUNID+'-'+sample+'-'+'-manual-ref-list.csv'

    if (inputArgs.manualRef):
      mandf = pd.DataFrame()
      for manRef in inputArgs.manualRef:
        tmp = {'sample': [sample], 'ref': [manRef], 'target': ['Manual reference']}
        tmpdf = pd.DataFrame.from_dict(tmp)
        mandf = mandf.append(tmpdf, ignore_index=True)
      mandf.to_csv(manualList,header=False,index=False)

    if os.path.exists(manualList) and os.path.getsize(manualList) > 0:
      tmp = pd.read_csv(manualList, names = ['sample','ref','target'])
      blastedList = blastedList.append(tmp[['sample','ref','target']])

  blastedList.drop_duplicates(inplace = True) 
  blastedList[['sample','ref','target']].to_csv(ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/'+RUNID+'-all-blasted-list.csv', header=False,index=False)

  logger.info('Blasted list considered:')
  with open(ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/'+RUNID+'-all-blasted-list.csv', 'r') as f:
    lines = f.readlines() 
    for line in lines:
      logger.info(''.join(line.split()))

    
  consensus_command = 'snakemake --snakefile '+scriptPath+'/06.consensusA.snakefile \
  --configfile '+ANALYSIS_FOLDER+'/config.yaml --config blastlist='+ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/'+RUNID+'-all-blasted-list.csv steps='+cleanOpts+' --use-conda --conda-prefix '+home+'/opt/iflow \
  --cores '+ str(inputArgs.threads) 

  with subprocess.Popen(consensus_command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell = True,bufsize=1, universal_newlines=True) as p:
      for line in p.stdout:
          logger.info(line.rstrip()) 

  consensus_command = 'snakemake --snakefile '+scriptPath+'/06.consensusC.merge.snakefile \
  --configfile '+ANALYSIS_FOLDER+'/config.yaml --config blastlist='+ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/'+RUNID+'-all-blasted-list.csv steps='+cleanOpts+' \
  --cores '+ str(inputArgs.threads) 

  with subprocess.Popen(consensus_command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell = True,bufsize=1, universal_newlines=True) as p:
    for line in p.stdout:
      logger.info(line.rstrip()) 

  fileHandler = logging.FileHandler(analysis_log_file)
  fileHandler.setFormatter(logFormatter)
  logger.addHandler(fileHandler)
  logger.info('Consensus generation finished')
logger.info('Analysis finished')

  ###################################################################################################################################################################

if (inputArgs.keep_bam):
  df = pd.read_csv(ANALYSIS_FOLDER + '/table.csv', names = ['sample'])

  SAMPLES = df['sample']

  consensus_log_file = ANALYSIS_FOLDER+'/'+RUNID+'_RESULTS/logfiles/consensus-'+nowTime+'.log'
  logger.info('Consensus generation')
  logger.info('Consensus log file: '+consensus_log_file)
  fileHandler = logging.FileHandler(consensus_log_file)
  fileHandler.setFormatter(logFormatter)
  logger.addHandler(fileHandler)
  blastedList = pd.DataFrame()

  for sample in SAMPLES:
    manualList = ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/'+sample+'/'+RUNID+'-'+sample+'-'+'-manual-ref-list.csv'
    if (inputArgs.manualRef):
      mandf = pd.DataFrame()
      for manRef in inputArgs.manualRef:
        tmp = {'sample': [sample], 'ref': [manRef], 'target': ['Manual reference']}
        tmpdf = pd.DataFrame.from_dict(tmp)
        mandf = mandf.append(tmpdf, ignore_index=True)
        mandf.to_csv(manualList,header=False,index=False)

    if os.path.exists(manualList) and os.path.getsize(manualList) > 0:
      tmp = pd.read_csv(manualList, names = ['sample','ref','target'])
      blastedList = blastedList.append(tmp[['sample','ref','target']])

  blastedList.drop_duplicates(inplace = True) 
  blastedList[['sample','ref','target']].to_csv(ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/'+RUNID+'-all-blasted-list.csv', header=False,index=False)

  logger.info('Blasted list considered:')
  with open(ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/'+RUNID+'-all-blasted-list.csv', 'r') as f:
    lines = f.readlines() 
    for line in lines:
      logger.info(''.join(line.split()))

  consensus_command = 'snakemake --snakefile '+scriptPath+'/06.consensusD-keep-bam.snakefile \
  --configfile '+ANALYSIS_FOLDER+'/config.yaml --config blastlist='+ANALYSIS_FOLDER + '/' + RUNID + '_RESULTS/'+RUNID+'-all-blasted-list.csv  \
  --cores '+ str(inputArgs.threads) 

  with subprocess.Popen(consensus_command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell = True,bufsize=1, universal_newlines=True) as p:
    for line in p.stdout:
      logger.info(line.rstrip()) 

  fileHandler = logging.FileHandler(analysis_log_file)
  fileHandler.setFormatter(logFormatter)
  logger.addHandler(fileHandler)
  logger.info('Consensus generation finished')
logger.info('Analysis finished')

  ###################################################################################################################################################################

exit()
